"""
Evaluation Functions
====================
ëª¨ë¸ í‰ê°€ë¥¼ ìœ„í•œ í•¨ìˆ˜ë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤.
"""

import time
import dspy
from dspy.evaluate import Evaluate
from typing import List, Dict, Any, Optional

from .metric import text_to_sql_metric, normalized_metric, MAX_SCORE
from .logger import PerformanceTracker


def evaluate_model(
    module: dspy.Module, 
    examples: List[dspy.Example], 
    verbose: bool = True
) -> dict:
    """
    ëª¨ë¸ í‰ê°€ ë° ìƒì„¸ ê²°ê³¼ ë°˜í™˜
    
    Args:
        module: í‰ê°€í•  DSPy ëª¨ë“ˆ
        examples: í‰ê°€ ë°ì´í„°ì…‹
        verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
    
    Returns:
        í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    results = {
        'total': len(examples),
        'perfect_match': 0,      # 3.5ì 
        'result_match': 0,       # 3.0ì 
        'partial_match': 0,      # 0.5 ~ 2.5ì 
        'wrong_result': 0,       # 0.5ì 
        'error': 0,              # 0ì 
        'scores': [],
        'details': []
    }
    
    for i, example in enumerate(examples):
        try:
            prediction = module(
                question=example.question,
                table_schema=example.table_schema,
                hint=getattr(example, 'hint', '')
            )
            
            score = text_to_sql_metric(example, prediction)
            results['scores'].append(score)
            
            # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
            if score >= 3.4:
                results['perfect_match'] += 1
                category = "âœ… Perfect"
            elif score >= 2.9:
                results['result_match'] += 1
                category = "ğŸŸ¢ Result Match"
            elif score >= 0.6:
                results['partial_match'] += 1
                category = f"ğŸŸ¡ Partial ({score:.2f})"
            elif score >= 0.4:
                results['wrong_result'] += 1
                category = "ğŸŸ  Wrong Result"
            else:
                results['error'] += 1
                category = "âŒ Error"
            
            if verbose and i < 5:
                print(f"\n[{i+1}] {category}")
                print(f"Q: {example.question[:80]}...")
                print(f"Pred SQL: {getattr(prediction, 'sql_query', '')[:80]}...")
                print(f"Score: {score}")
            
            results['details'].append({
                'question': example.question,
                'pred_sql': getattr(prediction, 'sql_query', ''),
                'gold_sql': example.gold_sql,
                'score': score,
                'category': category
            })
            
        except Exception as e:
            results['scores'].append(0)
            results['error'] += 1
            results['details'].append({
                'question': example.question,
                'error': str(e),
                'score': 0,
                'category': "âŒ Error"
            })
    
    # í†µê³„ ê³„ì‚°
    results['avg_score'] = sum(results['scores']) / len(results['scores']) if results['scores'] else 0
    results['avg_normalized'] = results['avg_score'] / MAX_SCORE
    
    _print_evaluation_summary(results)
    return results


def evaluate_with_dspy(
    module: dspy.Module, 
    examples: List[dspy.Example],
    tracker: Optional[PerformanceTracker] = None,
    step_name: str = "",
    num_threads: int = 1,
    display_progress: bool = True
) -> dict:
    """
    DSPy Evaluateë¥¼ ì‚¬ìš©í•œ í‰ê°€ + íŠ¸ë˜í‚¹
    
    Args:
        module: í‰ê°€í•  DSPy ëª¨ë“ˆ
        examples: í‰ê°€ ë°ì´í„°ì…‹
        tracker: PerformanceTracker ì¸ìŠ¤í„´ìŠ¤
        step_name: ìŠ¤í… ì´ë¦„
        num_threads: ë³‘ë ¬ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ìˆ˜
        display_progress: ì§„í–‰ ìƒí™© í‘œì‹œ ì—¬ë¶€
    
    Returns:
        í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    # ëª¨ë“ˆ ê²€ì¦
    if module is None:
        raise ValueError("í‰ê°€í•  ëª¨ë“ˆì´ Noneì…ë‹ˆë‹¤. ìµœì í™”ê°€ ì œëŒ€ë¡œ ì™„ë£Œë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    start_time = time.time()
    
    # DSPy Evaluate ìƒì„±
    evaluator = Evaluate(
        devset=examples,
        metric=normalized_metric,
        num_threads=num_threads,
        display_progress=display_progress,
        provide_traceback=True  # ìƒì„¸ ì—ëŸ¬ traceback í‘œì‹œ
    )
    
    # í‰ê°€ ì‹¤í–‰
    try:
        eval_result = evaluator(module)
    except Exception as e:
        print(f"âš ï¸ DSPy Evaluate ì—ëŸ¬: {e}")
        # Fallback: ìˆ˜ë™ í‰ê°€
        return _fallback_evaluation(module, examples, step_name)
    elapsed = time.time() - start_time
    
    # DSPy ê²°ê³¼ì—ì„œ í‰ê·  ì ìˆ˜ ì¶”ì¶œ (0~1 ë²”ìœ„ë¡œ ì •ê·œí™”)
    dspy_avg_score = 0.0
    raw_score = 0.0
    
    if hasattr(eval_result, 'score'):
        raw_score = float(eval_result.score) if eval_result.score is not None else 0.0
    elif isinstance(eval_result, (int, float)):
        raw_score = float(eval_result)
    
    # DSPy ì ìˆ˜ ì •ê·œí™”: í•­ìƒ 0~1 ë²”ìœ„ë¡œ ë³€í™˜
    # - 0~1: ì´ë¯¸ ì •ê·œí™”ë¨
    # - 1~100: ë°±ë¶„ìœ¨
    # - 100+: ì´ì  (ìƒ˜í”Œ ìˆ˜ë¡œ ë‚˜ëˆ”)
    if raw_score > 100:
        # ì´ì ì¸ ê²½ìš° (ì˜ˆ: 17.3 / 47 = 0.368)
        dspy_avg_score = raw_score / len(examples) if len(examples) > 0 else 0.0
    elif raw_score > 1.0:
        # ë°±ë¶„ìœ¨ì¸ ê²½ìš° (ì˜ˆ: 36.8 â†’ 0.368)
        dspy_avg_score = raw_score / 100.0
    else:
        # ì´ë¯¸ ì •ê·œí™”ë¨ (0~1)
        dspy_avg_score = raw_score
    
    # ìµœì¢… ë²”ìœ„ ì œí•œ (0~1)
    dspy_avg_score = max(0.0, min(1.0, dspy_avg_score))
    
    # ìƒì„¸ ê²°ê³¼ ë¶„ì„
    results = _analyze_evaluation_results(module, examples, eval_result, dspy_avg_score)
    results['elapsed_time'] = elapsed
    
    # íŠ¸ë˜í‚¹ ë¡œê·¸
    if tracker:
        tracker.log_step(results, step_name)
    
    _print_dspy_evaluation_summary(results, step_name)
    return results


def _analyze_evaluation_results(
    module: dspy.Module, 
    examples: List[dspy.Example], 
    eval_result: Any,
    dspy_avg_score: float = 0.0
) -> dict:
    """í‰ê°€ ê²°ê³¼ ë¶„ì„"""
    results = {
        'total': len(examples),
        'perfect_match': 0,
        'result_match': 0,
        'partial_match': 0,
        'wrong_result': 0,
        'error': 0,
        'scores': [],
        'details': []
    }
    
    # ê°œë³„ ê²°ê³¼ ì¶”ì¶œ ì‹œë„
    individual_results = None
    if hasattr(eval_result, 'results') and eval_result.results:
        individual_results = eval_result.results
    elif hasattr(eval_result, 'outputs') and eval_result.outputs:
        individual_results = eval_result.outputs
    
    parsed_successfully = False
    
    if individual_results:
        for i, result_item in enumerate(individual_results):
            # ë‹¤ì–‘í•œ DSPy ë²„ì „ í˜¸í™˜
            if hasattr(result_item, 'example'):
                example = result_item.example
            elif i < len(examples):
                example = examples[i]
            else:
                continue
            
            # prediction/output ì¶”ì¶œ
            output = None
            if hasattr(result_item, 'prediction'):
                output = result_item.prediction
            elif hasattr(result_item, 'output'):
                output = result_item.output
            
            # score ì¶”ì¶œ
            norm_score = 0
            if hasattr(result_item, 'score'):
                norm_score = result_item.score if result_item.score is not None else 0
            elif hasattr(result_item, 'metric'):
                norm_score = result_item.metric if result_item.metric is not None else 0
            
            if norm_score > 0:
                parsed_successfully = True
            
            original_score = norm_score * MAX_SCORE
            results['scores'].append(original_score)
            
            category = _categorize_score(original_score)
            _update_category_count(results, original_score)
            
            results['details'].append({
                'question': str(getattr(example, 'question', ''))[:100],
                'pred_sql': str(getattr(output, 'sql_query', ''))[:100] if output else '',
                'score': original_score,
                'category': category
            })
    
    # DSPy í‰ê·  ì ìˆ˜ê°€ ìˆê³ , ê°œë³„ íŒŒì‹±ì´ ì‹¤íŒ¨í•œ ê²½ìš° DSPy ì ìˆ˜ ì‚¬ìš©
    if dspy_avg_score > 0 and not parsed_successfully:
        # DSPyê°€ ë³´ê³ í•œ í‰ê·  ì ìˆ˜ ì‚¬ìš© (0~1 ë²”ìœ„)
        avg_score = dspy_avg_score * MAX_SCORE
        results['avg_score'] = avg_score
        results['avg_normalized'] = dspy_avg_score
        
        # ëŒ€ëµì ì¸ ë¶„í¬ ì¶”ì • (ì ìˆ˜ ê¸°ë°˜)
        # ì˜ˆ: 36.8% â†’ ì•½ 17ê°œ ì„±ê³µ, 30ê°œ ì‹¤íŒ¨
        total = len(examples)
        estimated_success = int(total * dspy_avg_score)
        estimated_success = max(0, min(estimated_success, total))  # 0~total ë²”ìœ„ ì œí•œ
        
        results['partial_match'] = estimated_success
        results['error'] = max(0, total - estimated_success)  # ìŒìˆ˜ ë°©ì§€
        results['scores'] = [avg_score] * total
        
        print(f"   â„¹ï¸ DSPy í‰ê·  ì ìˆ˜ ì‚¬ìš©: {dspy_avg_score:.3f} ({dspy_avg_score*100:.1f}%)")
        return results
    
    # ê°œë³„ ê²°ê³¼ê°€ ìˆìœ¼ë©´ í‰ê·  ê³„ì‚°
    if results['scores']:
        calculated_avg = sum(results['scores']) / len(results['scores'])
        # DSPy ì ìˆ˜ê°€ ë” ì‹ ë¢°í•  ìˆ˜ ìˆìœ¼ë©´ ì‚¬ìš©
        if dspy_avg_score > 0 and abs(calculated_avg - dspy_avg_score * MAX_SCORE) > 0.5:
            results['avg_score'] = dspy_avg_score * MAX_SCORE
            results['avg_normalized'] = dspy_avg_score
        else:
            results['avg_score'] = calculated_avg
            results['avg_normalized'] = calculated_avg / MAX_SCORE
    else:
        # ì ìˆ˜ ì—†ìœ¼ë©´ DSPy ì ìˆ˜ ì‚¬ìš©
        results['avg_score'] = dspy_avg_score * MAX_SCORE
        results['avg_normalized'] = dspy_avg_score
    
    return results


def _fallback_evaluation(
    module: dspy.Module,
    examples: List[dspy.Example],
    step_name: str = ""
) -> dict:
    """DSPy Evaluate ì‹¤íŒ¨ ì‹œ ìˆ˜ë™ í‰ê°€"""
    print("ğŸ”„ Fallback í‰ê°€ ëª¨ë“œë¡œ ì „í™˜...")
    
    results = {
        'total': len(examples),
        'perfect_match': 0,
        'result_match': 0,
        'partial_match': 0,
        'wrong_result': 0,
        'error': 0,
        'scores': [],
        'details': [],
        'elapsed_time': 0
    }
    
    start_time = time.time()
    
    for i, example in enumerate(examples):
        try:
            output = module(
                question=example.question,
                table_schema=example.table_schema,
                hint=getattr(example, 'hint', '')
            )
            norm_score = normalized_metric(example, output)
        except Exception as e:
            print(f"  âš ï¸ Example {i+1} ì—ëŸ¬: {str(e)[:50]}")
            output = None
            norm_score = 0
        
        original_score = norm_score * MAX_SCORE
        results['scores'].append(original_score)
        
        _update_category_count(results, original_score)
        
        results['details'].append({
            'question': str(example.question)[:100],
            'pred_sql': str(getattr(output, 'sql_query', ''))[:100] if output else '',
            'score': original_score,
            'category': _categorize_score(original_score)
        })
    
    results['elapsed_time'] = time.time() - start_time
    results['avg_score'] = sum(results['scores']) / len(results['scores']) if results['scores'] else 0
    results['avg_normalized'] = results['avg_score'] / MAX_SCORE
    
    _print_dspy_evaluation_summary(results, step_name)
    return results


def _categorize_score(score: float) -> str:
    """ì ìˆ˜ë¥¼ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜"""
    if score >= 3.4:
        return "Perfect"
    elif score >= 2.9:
        return "Result Match"
    elif score >= 0.6:
        return "Partial"
    elif score >= 0.4:
        return "Wrong Result"
    else:
        return "Error"


def _update_category_count(results: dict, score: float):
    """ì¹´í…Œê³ ë¦¬ë³„ ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸"""
    if score >= 3.4:
        results['perfect_match'] += 1
    elif score >= 2.9:
        results['result_match'] += 1
    elif score >= 0.6:
        results['partial_match'] += 1
    elif score >= 0.4:
        results['wrong_result'] += 1
    else:
        results['error'] += 1


def _print_evaluation_summary(results: dict):
    """í‰ê°€ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    total = results['total']
    print(f"\n{'='*50}")
    print(f"ğŸ“Š í‰ê°€ ê²°ê³¼ ìš”ì•½")
    print(f"{'='*50}")
    print(f"ì´ ìƒ˜í”Œ: {total}")
    print(f"âœ… Perfect Match (3.5ì ): {results['perfect_match']} ({results['perfect_match']/total*100:.1f}%)")
    print(f"ğŸŸ¢ Result Match (3.0ì ): {results['result_match']} ({results['result_match']/total*100:.1f}%)")
    print(f"ğŸŸ¡ Partial Match (0.5 ~ 2.5ì ): {results['partial_match']} ({results['partial_match']/total*100:.1f}%)")
    print(f"ğŸŸ  Wrong Result (0.5ì ): {results['wrong_result']} ({results['wrong_result']/total*100:.1f}%)")
    print(f"âŒ Error (0ì ): {results['error']} ({results['error']/total*100:.1f}%)")
    print(f"\ní‰ê·  ì ìˆ˜: {results['avg_score']:.3f}")
    print(f"ì •ê·œí™” ì ìˆ˜: {results['avg_normalized']:.3f}")


def _print_dspy_evaluation_summary(results: dict, step_name: str):
    """DSPy í‰ê°€ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    total = results['total']
    elapsed = results.get('elapsed_time', 0)
    
    print(f"\n{'='*50}")
    print(f"ğŸ“Š DSPy Evaluate ê²°ê³¼ - {step_name}")
    print(f"{'='*50}")
    print(f"âœ… Perfect Match (3.5): {results['perfect_match']} ({results['perfect_match']/total*100:.1f}%)")
    print(f"ğŸŸ¢ Result Match (3.0): {results['result_match']} ({results['result_match']/total*100:.1f}%)")
    print(f"ğŸŸ¡ Partial Match (0.5 ~ 2.5): {results['partial_match']} ({results['partial_match']/total*100:.1f}%)")
    print(f"ğŸŸ  Wrong Result (0.5): {results['wrong_result']} ({results['wrong_result']/total*100:.1f}%)")
    print(f"âŒ Error (0.0): {results['error']} ({results['error']/total*100:.1f}%)")
    print(f"\nâ±ï¸  ì‹¤í–‰ ì‹œê°„: {elapsed:.1f}ì´ˆ")
    print(f"ğŸ¯ í‰ê·  ì ìˆ˜: {results['avg_score']:.3f}")
    print(f"ğŸ“Š ì •ê·œí™” ì ìˆ˜: {results['avg_normalized']:.3f}")


def compare_results(
    baseline_results: dict, 
    optimized_results: dict,
    show_improvement: bool = True
) -> dict:
    """ë² ì´ìŠ¤ë¼ì¸ê³¼ ìµœì í™” ê²°ê³¼ ë¹„êµ"""
    comparison = {
        'baseline': baseline_results,
        'optimized': optimized_results,
        'improvement': {}
    }
    
    for key in ['avg_score', 'perfect_match', 'result_match', 'partial_match']:
        baseline_val = baseline_results.get(key, 0)
        optimized_val = optimized_results.get(key, 0)
        comparison['improvement'][key] = optimized_val - baseline_val
    
    if show_improvement:
        print("\n" + "="*60)
        print("ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ: Baseline vs Optimized")
        print("="*60)
        print(f"í‰ê·  ì ìˆ˜: {baseline_results['avg_score']:.3f} â†’ {optimized_results['avg_score']:.3f} "
              f"({comparison['improvement']['avg_score']:+.3f})")
        print(f"Perfect Match: {baseline_results['perfect_match']} â†’ {optimized_results['perfect_match']} "
              f"({comparison['improvement']['perfect_match']:+d})")
    
    return comparison


print("âœ… í‰ê°€ í•¨ìˆ˜ ë¡œë“œ ì™„ë£Œ")

