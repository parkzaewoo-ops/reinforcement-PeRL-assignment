import dspy
from dspy.evaluate import Evaluate
import os
import time
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')  # Non-interactive backend for saving files
from typing import Optional, Tuple, Any, List, Dict
from sklearn.model_selection import train_test_split
from datasets import load_dataset
from core.utiles import get_db_schema, execute_sql, compare_sql, compare_results, calculate_evidence_similarity
from core.logger import PerformanceTracker, StepMetrics
from core.logger import list_saved_results, load_and_display_history, display_latest_graph

# í•œê¸€ í°íŠ¸ ì„¤ì • (matplotlib)
plt.rcParams['font.family'] = ['DejaVu Sans', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False

# ê²°ê³¼ ì €ì¥ ê²½ë¡œ
RESULTS_DIR = "/data/workspace/sogang/sqlagent/results"
os.makedirs(RESULTS_DIR, exist_ok=True)
SQLITE_PATH = "/data/workspace/sogang/sqlagent/data/BIRD.sqlite"
SCHEMA_DIR = "/data/workspace/sogang/sqlagent/data"
ds = load_dataset("birdsql/bird_sql_dev_20251106")
df = ds['dev_20251106'].to_pandas()
hard_df = df[df['difficulty'] == 'challenging'].copy()
train_df, test_df = train_test_split(hard_df, test_size=0.2, random_state=42)
print(f"Train: {len(train_df)}, Test: {len(test_df)}")

model_name = "Qwen/Qwen3-Next-80B-A3B-Instruct"
api_key = "token-abc123"
model = dspy.LM(
    model=f"openai/{model_name}", 
    api_base="http://211.47.56.81:7972/v1", 
    api_key=api_key, 
    temperature=0.6,
    request_kwargs={
        "timeout": 60.0,
        "max_retries": 3,
        "max_tokens": 30000
    }
)
dspy.settings.configure(lm=model)
print(f"ëª¨ë¸ ì„¤ì • ì™„ë£Œ: {model_name}")

class TextToSQLSignature(dspy.Signature):
    """You are an expert SQL developer. Given a natural language question and database schema, 
    generate a correct SQL query that answers the question. 
    Think step by step about the table relationships and required columns."""
    
    # Input fields (MIPROv2ê°€ ì´ Signatureì˜ docstringì„ System Promptë¡œ ìµœì í™”í•¨)
    question = dspy.InputField(desc="ìì—°ì–´ë¡œ ëœ ë°ì´í„°ë² ì´ìŠ¤ ì§ˆì˜ ì§ˆë¬¸")
    table_schema = dspy.InputField(desc="ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ (CREATE TABLE ë¬¸)")
    hint = dspy.InputField(desc="ì§ˆë¬¸ í•´ì„ì„ ìœ„í•œ íŒíŠ¸/ì¦ê±°", default="")
    
    # Output fields
    reasoning = dspy.OutputField(desc="SQL ì¿¼ë¦¬ë¥¼ ì‘ì„±í•˜ê¸° ìœ„í•œ ë‹¨ê³„ë³„ ì¶”ë¡  ê³¼ì •")
    sql_query = dspy.OutputField(desc="ìµœì¢… SQL ì¿¼ë¦¬ (SELECT ë¬¸)")
    evidence = dspy.OutputField(desc="ì¿¼ë¦¬ ê²°ê³¼ë¥¼ ì„¤ëª…í•˜ëŠ” ì¦ê±° ë¬¸ì¥")


class TextToSQLModule(dspy.Module):
    """Text-to-SQL ë³€í™˜ ëª¨ë“ˆ (ìµœì í™” ëŒ€ìƒ)"""
    def __init__(self):
        super().__init__()
        self.cot = dspy.ChainOfThought(TextToSQLSignature)
    
    def forward(self, question: str, table_schema: str, hint: str = "") -> dspy.Prediction:
        prediction = self.cot(
            question=question,
            table_schema=table_schema,
            hint=hint
        )
        return dspy.Prediction(
            reasoning=prediction.reasoning,
            sql_query=prediction.sql_query,
            evidence=prediction.evidence
        )


print("âœ… Signature & Module ì •ì˜ ì™„ë£Œ")
print("   ğŸ“ MIPROv2ëŠ” Signatureì˜ docstringì„ System Promptë¡œ ìµœì í™”í•©ë‹ˆë‹¤.")
# ============================================
# 7. DSPy Example ë°ì´í„°ì…‹ ì¤€ë¹„
# ============================================

def create_dspy_examples(df: pd.DataFrame) -> List[dspy.Example]:
    examples = []
    schema = get_db_schema("bird")    
    for _, row in df.iterrows():
        example = dspy.Example(
            question=row['question'],
            table_schema=schema,
            hint=row['evidence'] if pd.notna(row['evidence']) else "",
            gold_sql=row['SQL'],
            gold_evidence=row['evidence'] if pd.notna(row['evidence']) else "",
        ).with_inputs('question', 'table_schema', 'hint')
        examples.append(example)
    return examples


train_examples = create_dspy_examples(train_df)
test_examples = create_dspy_examples(test_df)

print(f"Train examples: {len(train_examples)}")
print(f"Test examples: {len(test_examples)}")
print(f"\nìƒ˜í”Œ example:")
print(f"Question: {train_examples[0].question[:100]}...")
print(f"Gold SQL: {train_examples[0].gold_sql[:100]}...")
# ============================================
# 6. ë³´ìƒ ì²´ê³„ ê¸°ë°˜ ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ í•¨ìˆ˜
# ============================================
"""
ë³´ìƒ ì²´ê³„:
- Queryì™€ DB ì¡°íšŒ ê²°ê³¼ê°€ ì™„ë²½íˆ ì¼ì¹˜: 2.0ì 
- QueryëŠ” ë‹¤ë¥´ì§€ë§Œ DB ê²°ê³¼ê°€ ë™ì¼: 1.5ì 
- Query/ê²°ê³¼ ë‹¤ë¥´ì§€ë§Œ evidence ìœ ì‚¬ë„ ê¸°ë°˜: 0.0 ~ 1.0ì 
- ì‹¤í–‰ì€ ë˜ì§€ë§Œ ê²°ê³¼ê°€ í‹€ë¦¼: -0.5ì 
- ì—ëŸ¬ ë°œìƒ: -1.0ì 
"""

def text_to_sql_metric(example, prediction, trace=None) -> float:
    """
    ë³´ìƒ ì²´ê³„:
    - Query + ê²°ê³¼ ì™„ë²½ ì¼ì¹˜: 3.5ì 
    - Query ë‹¤ë¥´ì§€ë§Œ ê²°ê³¼ ë™ì¼: 3.0ì 
    - Evidence ìœ ì‚¬ë„ ê¸°ë°˜: 0.5 ~ 2.5ì 
    - ì‹¤í–‰ë˜ì§€ë§Œ ê²°ê³¼ í‹€ë¦¼: 0.5ì 
    - ì—ëŸ¬ ë°œìƒ: 0.0ì 
    """
    gold_sql = example.gold_sql
    gold_evidence = example.gold_evidence if hasattr(example, 'gold_evidence') else ""
    pred_sql = prediction.sql_query if hasattr(prediction, 'sql_query') else ""
    pred_evidence = prediction.evidence if hasattr(prediction, 'evidence') else ""
    
    # 1. ì˜ˆì¸¡ëœ SQL ì‹¤í–‰
    pred_success, pred_result = execute_sql(pred_sql)
    
    # ì—ëŸ¬ ë°œìƒ ì‹œ 0.0ì 
    if not pred_success:
        return 0.0
    
    # 2. Gold SQL ì‹¤í–‰
    gold_success, gold_result = execute_sql(gold_sql)
    if not gold_success:
        return 0.5  # Gold SQL ì—ëŸ¬ì§€ë§Œ ì˜ˆì¸¡ì€ ì‹¤í–‰ë¨
    
    sql_match = compare_sql(pred_sql, gold_sql)
    result_match = compare_results(pred_result, gold_result)
    
    # 3. ë³´ìƒ ê³„ì‚°
    if sql_match and result_match:
        # Query + ê²°ê³¼ ì™„ë²½ ì¼ì¹˜
        return 3.5
    elif result_match:
        # QueryëŠ” ë‹¤ë¥´ì§€ë§Œ ê²°ê³¼ ë™ì¼
        return 3.0
    else:
        # ê²°ê³¼ê°€ ë‹¤ë¦„ - evidence ìœ ì‚¬ë„ë¡œ ë¶€ë¶„ ì ìˆ˜ (0.5 ~ 2.5)
        evidence_sim = calculate_evidence_similarity(pred_evidence, gold_evidence)
        if evidence_sim > 0.1:
            # evidence_sim (0~1) â†’ 0.5 ~ 2.5ë¡œ ìŠ¤ì¼€ì¼ë§
            return 0.5 + evidence_sim * 2.0
        else:
            # ì‹¤í–‰ë˜ì§€ë§Œ ê²°ê³¼ í‹€ë¦¼
            return 0.5

def normalized_metric(example, prediction, trace=None) -> float:
    """
    ì •ê·œí™”ëœ ë©”íŠ¸ë¦­ (0~1 ë²”ìœ„ë¡œ ë³€í™˜) - DSPy ìµœì í™”ìš©
    
    DSPyì˜ COPRO/MIPROv2/EvaluateëŠ” 0~1 ë²”ìœ„ì˜ ë©”íŠ¸ë¦­ì„ ê¸°ëŒ€í•©ë‹ˆë‹¤.
    0~3.5 ì ìˆ˜ë¥¼ 0~1ë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤.
    """
    score = text_to_sql_metric(example, prediction, trace)
    # 0~3.5 â†’ 0~1 ì •ê·œí™”
    return score / 3.5


print("ë©”íŠ¸ë¦­ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ")
print("   - text_to_sql_metric(): ì›ë³¸ ì ìˆ˜ (0 ~ 3.5)")
print("   - normalized_metric(): DSPy ìµœì í™”ìš© ì •ê·œí™” (0 ~ 1)")
# ============================================
# 8. í‰ê°€ í•¨ìˆ˜
# ============================================

def evaluate_model(module: dspy.Module, examples: List[dspy.Example], 
                   verbose: bool = True) -> dict:
    """ëª¨ë¸ í‰ê°€ ë° ìƒì„¸ ê²°ê³¼ ë°˜í™˜"""
    
    results = {
        'total': len(examples),
        'perfect_match': 0,      # 3.5ì 
        'result_match': 0,       # 3ì 
        'partial_match': 0,      # 0.5 ~ 2.5ì 
        'wrong_result': 0,       # 0.5ì 
        'error': 0,              # 0ì 
        'scores': [],
        'details': []
    }
    
    for i, example in enumerate(examples):
        try:
            prediction = module(
                question=example.question,
                table_schema=example.table_schema,
                hint=example.hint
            )
            
            score = text_to_sql_metric(example, prediction)
            results['scores'].append(score)
            
            # ìƒˆë¡œìš´ ë³´ìƒ ì²´ê³„ (0 ~ 3.5) ê¸°ì¤€ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
            if score >= 3.4:
                results['perfect_match'] += 1
                category = "âœ… Perfect"
            elif score >= 2.9:
                results['result_match'] += 1
                category = "ğŸŸ¢ Result Match"
            elif score >= 0.6:
                results['partial_match'] += 1
                category = f"ğŸŸ¡ Partial ({score:.2f})"
            elif score >= 0.4:
                results['wrong_result'] += 1
                category = "ğŸŸ  Wrong Result"
            else:
                results['error'] += 1
                category = "âŒ Error"
            
            if verbose and i < 5:  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥
                print(f"\n[{i+1}] {category}")
                print(f"Q: {example.question[:80]}...")
                print(f"Pred SQL: {prediction.sql_query[:80]}...")
                print(f"Score: {score}")
            
            results['details'].append({
                'question': example.question,
                'pred_sql': prediction.sql_query,
                'gold_sql': example.gold_sql,
                'score': score,
                'category': category
            })
            
        except Exception as e:
            results['scores'].append(0)
            results['error'] += 1
            results['details'].append({
                'question': example.question,
                'error': str(e),
                'score': 0,
                'category': "âŒ Error"
            })
    
    # í†µê³„ ê³„ì‚°
    results['avg_score'] = sum(results['scores']) / len(results['scores']) if results['scores'] else 0
    results['avg_normalized'] = results['avg_score'] / 3.5  # 0~3.5 â†’ 0~1 ì •ê·œí™”
    
    print(f"\n{'='*50}")
    print(f"ğŸ“Š í‰ê°€ ê²°ê³¼ ìš”ì•½")
    print(f"{'='*50}")
    print(f"ì´ ìƒ˜í”Œ: {results['total']}")
    print(f"âœ… Perfect Match (3.5ì ): {results['perfect_match']} ({results['perfect_match']/results['total']*100:.1f}%)")
    print(f"ğŸŸ¢ Result Match (3ì ): {results['result_match']} ({results['result_match']/results['total']*100:.1f}%)")
    print(f"ğŸŸ¡ Partial Match (0.5 ~ 2.5ì ): {results['partial_match']} ({results['partial_match']/results['total']*100:.1f}%)")
    print(f"ğŸŸ  Wrong Result (0.5ì ): {results['wrong_result']} ({results['wrong_result']/results['total']*100:.1f}%)")
    print(f"âŒ Error (0ì ): {results['error']} ({results['error']/results['total']*100:.1f}%)")
    print(f"\ní‰ê·  ì ìˆ˜: {results['avg_score']:.3f}")
    print(f"ì •ê·œí™” ì ìˆ˜: {results['avg_normalized']:.3f}")
    
    return results


print("í‰ê°€ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ")
# ============================================
# 9. DSPy ìµœì í™” íŒŒì´í”„ë¼ì¸ (COPRO / MIPROv2)
# ============================================
"""
ğŸ”¥ ë‘ ê°€ì§€ ìµœì í™” ë°©ë²•:

1ï¸âƒ£ COPRO (Collaborative Prompt Optimization)
   - Signatureì˜ ì„¤ëª…ë¬¸(instruction)ì„ ìë™ ê°œì„ 
   - ìš©ë„: ì„¤ëª…ë¬¸ë§Œ ìµœì í™”í•˜ê³ ì í•  ë•Œ

2ï¸âƒ£ MIPROv2 (Mixed Instruction and PRompt Optimization v2)
   - ë² ì´ì§€ì•ˆ ìµœì í™”ë¡œ ì„¤ëª…ë¬¸ + Few-shot ì˜ˆì œ ëª¨ë‘ ìµœì í™”
   - ìš©ë„: Zero-shotìœ¼ë¡œ ì‹œì‘í•˜ê±°ë‚˜ ì˜ˆì œê°€ 200ê°œ ì´ìƒì¸ ê²½ìš°
"""

def optimize_with_copro(
    module: dspy.Module,
    train_examples: List[dspy.Example],
    metric_fn,
    breadth: int = 10,
    depth: int = 1,
    init_temperature: float = 0.6,
    num_threads: int = 20
) -> dspy.Module:
    """
    COPROë¡œ ìµœì í™” - Signature ì„¤ëª…ë¬¸(Instruction)ë§Œ ìë™ ê°œì„ 
    
    COPROê°€ ìµœì í™”í•˜ëŠ” ê²ƒ:
    âœ… Instruction (System Prompt) - ì„¤ëª…ë¬¸ ìë™ ê°œì„ 
    Args:
        breadth: ê° ë‹¨ê³„ì—ì„œ ìƒì„±í•  í›„ë³´ ìˆ˜
        depth: ìµœì í™” ë°˜ë³µ íšŸìˆ˜
        init_temperature: ì´ˆê¸° temperature (ë‹¤ì–‘ì„± ì¡°ì ˆ)
    """
    try:
        from dspy.teleprompt import COPRO
        
        print("ğŸš€ COPRO ìµœì í™” ì‹œì‘...")
        print("   ğŸ“ ìµœì í™” ëŒ€ìƒ:")
        print("      âœ… Signature ì„¤ëª…ë¬¸ (Instruction)")
        print("      âŒ Few-shot demonstrations (ìµœì í™” ì•ˆí•¨)")
        print(f"   ğŸ”§ ì„¤ì •: breadth={breadth}, depth={depth}")
        
        optimizer = COPRO(
            metric=metric_fn,
            breadth=breadth,
            depth=depth,
            init_temperature=init_temperature,
            verbose=True
        )
        
        optimized_module = optimizer.compile(
            module,
            trainset=train_examples,
            eval_kwargs=dict(num_threads=num_threads, display_progress=True)
        )
        
        # ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ì¶œë ¥
        print("\n" + "="*60)
        print("ğŸ¯ COPRO ìµœì í™”ëœ Instruction í™•ì¸")
        print("="*60)
        display_optimized_prompt(optimized_module)
        
        print("\nâœ… COPRO ìµœì í™” ì™„ë£Œ!")
        return optimized_module
        
    except ImportError as e:
        print(f"âš ï¸ COPROë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        raise
    except Exception as e:
        print(f"âš ï¸ COPRO ì˜¤ë¥˜: {e}")
        raise


def optimize_with_mipro(
    module: dspy.Module,
    train_examples: List[dspy.Example],
    metric_fn,
    num_candidates: int = 10,
    init_temperature: float = 1.4,
    max_bootstrapped_demos: int = 3,
    max_labeled_demos: int = 4,
    num_threads: int = 20
) -> dspy.Module:
    """
    MIPROv2ë¡œ ìµœì í™” - ë² ì´ì§€ì•ˆ ìµœì í™”ë¡œ ì„¤ëª…ë¬¸ + Few-shot ëª¨ë‘ ìµœì í™”
    
    MIPROv2ê°€ ìµœì í™”í•˜ëŠ” ê²ƒ:
    âœ… Instruction (System Prompt) - ìì—°ì–´ ì§€ì‹œë¬¸ ìë™ ìƒì„±/ìµœì í™”
    âœ… Few-shot demonstrations - ìµœì ì˜ ì˜ˆì‹œ ì„ íƒ
    
    ê¶Œì¥ ì‚¬ìš© ì‹œì :
    - Zero-shotìœ¼ë¡œ ì‹œì‘í•  ë•Œ
    - ì˜ˆì œê°€ 200ê°œ ì´ìƒì¸ ê²½ìš°
    
    Args:
        num_candidates: ë² ì´ì§€ì•ˆ ìµœì í™” í›„ë³´ ìˆ˜
        init_temperature: ì´ˆê¸° temperature
        max_bootstrapped_demos: ë¶€íŠ¸ìŠ¤íŠ¸ë© ì˜ˆì‹œ ìˆ˜
        max_labeled_demos: ë¼ë²¨ë§ëœ ì˜ˆì‹œ ìˆ˜
    """
    try:
        from dspy.teleprompt import MIPROv2
        
        print("ğŸš€ MIPROv2 ìµœì í™” ì‹œì‘...")
        print("   ğŸ“ ìµœì í™” ëŒ€ìƒ:")
        print("      âœ… System Prompt (Instruction) - ë² ì´ì§€ì•ˆ ìµœì í™”")
        print("      âœ… Few-shot demonstrations")
        print(f"   ğŸ”§ ì„¤ì •: candidates={num_candidates}, demos={max_bootstrapped_demos}+{max_labeled_demos}")
        
        optimizer = MIPROv2(
            metric=metric_fn,
            auto=None,  # ìˆ˜ë™ ì„¤ì • ëª¨ë“œ (num_candidates/num_trials ì‚¬ìš© ì‹œ í•„ìˆ˜)
            num_candidates=num_candidates,
            init_temperature=init_temperature,
            max_bootstrapped_demos=max_bootstrapped_demos,
            max_labeled_demos=max_labeled_demos,
            num_threads=num_threads,
            verbose=True
        )
        
        optimized_module = optimizer.compile(
            module,
            trainset=train_examples,
            num_trials=num_candidates  # auto=Noneì¼ ë•Œ num_trials í•„ìˆ˜
        )
        
        # ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ì¶œë ¥
        print("\n" + "="*60)
        print("ğŸ¯ MIPROv2 ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ í™•ì¸")
        print("="*60)
        display_optimized_prompt(optimized_module)
        
        print("\nâœ… MIPROv2 ìµœì í™” ì™„ë£Œ!")
        return optimized_module
        
    except ImportError as e:
        print(f"âš ï¸ MIPROv2ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        raise
    except Exception as e:
        print(f"âš ï¸ MIPROv2 ì˜¤ë¥˜: {e}")
        raise


def display_optimized_prompt(module: dspy.Module):
    """ìµœì í™”ëœ ëª¨ë“ˆì˜ í”„ë¡¬í”„íŠ¸ ì¶œë ¥"""
    try:
        # ChainOfThought ë‚´ë¶€ì˜ predict ëª¨ë“ˆ ì ‘ê·¼
        if hasattr(module, 'cot'):
            predictor = module.cot
        elif hasattr(module, 'predict'):
            predictor = module.predict
        else:
            predictor = module
        
        # Extended Signatureì—ì„œ instructions ì¶”ì¶œ
        if hasattr(predictor, 'extended_signature'):
            sig = predictor.extended_signature
            if hasattr(sig, 'instructions'):
                print(f"\nğŸ“‹ ìµœì í™”ëœ Instruction:")
                print("-" * 50)
                print(sig.instructions)
                print("-" * 50)
        
        # Signatureì—ì„œ ì§ì ‘ ì¶”ì¶œ ì‹œë„
        if hasattr(predictor, 'signature'):
            sig = predictor.signature
            if hasattr(sig, 'instructions'):
                print(f"\nğŸ“‹ Signature Instruction:")
                print("-" * 50)
                print(sig.instructions)
                print("-" * 50)
        
        # Demos (Few-shot examples) ì¶œë ¥
        if hasattr(predictor, 'demos') and predictor.demos:
            print(f"\nğŸ“š ìµœì í™”ëœ Few-shot Demos ({len(predictor.demos)}ê°œ):")
            for i, demo in enumerate(predictor.demos[:3]):  # ì²˜ìŒ 3ê°œë§Œ
                print(f"\n  [Demo {i+1}]")
                if hasattr(demo, 'question'):
                    print(f"  Q: {str(demo.question)[:100]}...")
                if hasattr(demo, 'sql_query'):
                    print(f"  SQL: {str(demo.sql_query)[:100]}...")
        else:
            print("\nğŸ“š Few-shot Demos: ì—†ìŒ (Zero-shot)")
        
    except Exception as e:
        print(f"í”„ë¡¬í”„íŠ¸ ì¶œë ¥ ì˜¤ë¥˜: {e}")


def compare_prompts(baseline_module: dspy.Module, optimized_module: dspy.Module):
    """ë² ì´ìŠ¤ë¼ì¸ê³¼ ìµœì í™”ëœ ëª¨ë“ˆì˜ í”„ë¡¬í”„íŠ¸ ë¹„êµ"""
    print("\n" + "="*60)
    print("ğŸ“Š í”„ë¡¬í”„íŠ¸ ë¹„êµ: Baseline vs Optimized")
    print("="*60)
    
    print("\nğŸ”µ [Baseline]")
    display_optimized_prompt(baseline_module)
    
    print("\nğŸŸ¢ [Optimized]")
    display_optimized_prompt(optimized_module)


print("âœ… ìµœì í™” í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ")
print("=" * 50)
print("ğŸ”§ ì‚¬ìš© ê°€ëŠ¥í•œ ìµœì í™” ë°©ë²•:")
print("   1ï¸âƒ£ optimize_with_copro()  - Instructionë§Œ ìµœì í™”")
print("   2ï¸âƒ£ optimize_with_mipro()  - Instruction + Few-shot ëª¨ë‘ ìµœì í™” â­")
print("=" * 50)
# ============================================
# 9-1. DSPy Evaluate ê¸°ë°˜ í‰ê°€ í•¨ìˆ˜ (íŠ¸ë˜í‚¹ í†µí•©)
# ============================================

def evaluate_with_dspy(
    module: dspy.Module, 
    examples: List[dspy.Example],
    tracker: PerformanceTracker = None,
    step_name: str = "",
    num_threads: int = 1,
    display_progress: bool = True
) -> dict:
    """
    DSPy Evaluateë¥¼ ì‚¬ìš©í•œ í‰ê°€ + íŠ¸ë˜í‚¹ (ìƒˆë¡œìš´ API í˜¸í™˜)
    
    Args:
        module: í‰ê°€í•  DSPy ëª¨ë“ˆ
        examples: í‰ê°€ ë°ì´í„°ì…‹
        tracker: PerformanceTracker ì¸ìŠ¤í„´ìŠ¤
        step_name: ìŠ¤í… ì´ë¦„
        num_threads: ë³‘ë ¬ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ìˆ˜
        display_progress: ì§„í–‰ ìƒí™© í‘œì‹œ ì—¬ë¶€
    
    Returns:
        dict: í‰ê°€ ê²°ê³¼
    """
    start_time = time.time()
    
    # DSPy Evaluate ìƒì„± (normalized_metric ì‚¬ìš© - 0~1 ë²”ìœ„)
    evaluator = Evaluate(
        devset=examples,
        metric=normalized_metric,  # ì •ê·œí™”ëœ ë©”íŠ¸ë¦­ ì‚¬ìš©!
        num_threads=num_threads,
        display_progress=display_progress
    )
    
    # í‰ê°€ ì‹¤í–‰ - ìƒˆë¡œìš´ APIëŠ” EvaluationResult ê°ì²´ ë°˜í™˜
    eval_result = evaluator(module)
    
    elapsed = time.time() - start_time
    
    # EvaluationResultì—ì„œ ê²°ê³¼ ì¶”ì¶œ
    # ìƒˆë¡œìš´ API: eval_result.score (í‰ê· ), eval_result.results (ìƒì„¸)
    if hasattr(eval_result, 'score'):
        avg_score = eval_result.score
    else:
        avg_score = eval_result if isinstance(eval_result, (int, float)) else 0
    
    # ìƒì„¸ ê²°ê³¼ ë¶„ì„
    results = {
        'total': len(examples),
        'perfect_match': 0,
        'result_match': 0,
        'partial_match': 0,
        'wrong_result': 0,
        'error': 0,
        'scores': [],
        'details': []
    }
    
    # ìƒˆë¡œìš´ APIì—ì„œ ê°œë³„ ê²°ê³¼ ì¶”ì¶œ
    if hasattr(eval_result, 'results') and eval_result.results:
        for i, result_item in enumerate(eval_result.results):
            example = result_item.example if hasattr(result_item, 'example') else examples[i]
            output = result_item.prediction if hasattr(result_item, 'prediction') else None
            norm_score = result_item.score if hasattr(result_item, 'score') else 0
            
            # ì •ê·œí™”ëœ ì ìˆ˜(0~1)ë¥¼ ì›ë³¸ ì ìˆ˜(0~3.5)ë¡œ ë³µì›
            original_score = norm_score * 3.5
            results['scores'].append(original_score)
            
            # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
            # ìƒˆë¡œìš´ ë³´ìƒ ì²´ê³„ (0 ~ 3.5) ê¸°ì¤€ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
            if original_score >= 3.4:
                results['perfect_match'] += 1
                category = "Perfect"
            elif original_score >= 2.9:
                results['result_match'] += 1
                category = "Result Match"
            elif original_score >= 0.6:
                results['partial_match'] += 1
                category = "Partial"
            elif original_score >= 0.4:
                results['wrong_result'] += 1
                category = "Wrong Result"
            else:
                results['error'] += 1
                category = "Error"
            
            results['details'].append({
                'question': str(getattr(example, 'question', ''))[:100],
                'pred_sql': str(getattr(output, 'sql_query', ''))[:100] if output else '',
                'score': original_score,
                'category': category
            })
    else:
        # fallback: ìˆ˜ë™ í‰ê°€
        for i, example in enumerate(examples):
            try:
                output = module(
                    question=example.question,
                    table_schema=example.table_schema,
                    hint=getattr(example, 'hint', '')
                )
                norm_score = normalized_metric(example, output)  # ì •ê·œí™”ëœ ë©”íŠ¸ë¦­ ì‚¬ìš©
            except Exception as e:
                output = None
                norm_score = 0
            
            # ì •ê·œí™”ëœ ì ìˆ˜(0~1)ë¥¼ ì›ë³¸ ì ìˆ˜(0~3.5)ë¡œ ë³µì›
            original_score = norm_score * 3.5
            results['scores'].append(original_score)
            
            # ìƒˆë¡œìš´ ë³´ìƒ ì²´ê³„ (0 ~ 3.5) ê¸°ì¤€ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
            if original_score >= 3.4:
                results['perfect_match'] += 1
                category = "Perfect"
            elif original_score >= 2.9:
                results['result_match'] += 1
                category = "Result Match"
            elif original_score >= 0.6:
                results['partial_match'] += 1
                category = "Partial"
            elif original_score >= 0.4:
                results['wrong_result'] += 1
                category = "Wrong Result"
            else:
                results['error'] += 1
                category = "Error"
            
            results['details'].append({
                'question': str(example.question)[:100],
                'pred_sql': str(getattr(output, 'sql_query', ''))[:100] if output else '',
                'score': original_score,
                'category': category
            })
    
    results['avg_score'] = sum(results['scores']) / len(results['scores']) if results['scores'] else 0
    results['avg_normalized'] = results['avg_score'] / 3.5  # 0~3.5 â†’ 0~1 ì •ê·œí™”
    results['elapsed_time'] = elapsed
    
    # íŠ¸ë˜í‚¹ ë¡œê·¸
    if tracker:
        tracker.log_step(results, step_name)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\n{'='*50}")
    print(f"ğŸ“Š DSPy Evaluate ê²°ê³¼ - {step_name}")
    print(f"{'='*50}")
    print(f"âœ… Perfect Match (3.5): {results['perfect_match']} ({results['perfect_match']/results['total']*100:.1f}%)")
    print(f"ğŸŸ¢ Result Match (3.0): {results['result_match']} ({results['result_match']/results['total']*100:.1f}%)")
    print(f"ğŸŸ¡ Partial Match (0.5 ~ 2.5): {results['partial_match']} ({results['partial_match']/results['total']*100:.1f}%)")
    print(f"ğŸŸ  Wrong Result (0.5): {results['wrong_result']} ({results['wrong_result']/results['total']*100:.1f}%)")
    print(f"âŒ Error (0.0): {results['error']} ({results['error']/results['total']*100:.1f}%)")
    print(f"\nâ±ï¸  ì‹¤í–‰ ì‹œê°„: {elapsed:.1f}ì´ˆ")
    print(f"ğŸ¯ í‰ê·  ì ìˆ˜: {results['avg_score']:.3f}")
    print(f"ğŸ“Š ì •ê·œí™” ì ìˆ˜: {results['avg_normalized']:.3f}")
    
    return results


def run_optimization_with_tracking(
    module: dspy.Module,
    train_examples: List[dspy.Example],
    test_examples: List[dspy.Example],
    tracker: PerformanceTracker,
    optimizer_type: str = "mipro",  # ê¸°ë³¸ê°’ì„ miproë¡œ ë³€ê²½!
    save_intermediate: bool = True,
    breadth: int = 10,
    depth: int = 3,
    init_temperature: float = 0.6,
    num_threads: int = 10,
) -> Tuple[dspy.Module, dict]:
    """
    ìµœì í™” ê³¼ì •ì„ íŠ¸ë˜í‚¹í•˜ë©° ì‹¤í–‰
    
    Args:
        module: ìµœì í™”í•  ëª¨ë“ˆ
        train_examples: í•™ìŠµ ë°ì´í„°
        test_examples: í…ŒìŠ¤íŠ¸ ë°ì´í„°
        tracker: PerformanceTracker
        optimizer_type: "mipro" (System Prompt ìµœì í™”) ë˜ëŠ” "bootstrap" (Few-shotë§Œ)
        save_intermediate: ì¤‘ê°„ ê²°ê³¼ ì €ì¥ ì—¬ë¶€
    
    Returns:
        (optimized_module, final_results)
    """
    # ë² ì´ìŠ¤ë¼ì¸ ëª¨ë“ˆ ì €ì¥ (ë‚˜ì¤‘ì— ë¹„êµìš©)
    baseline_module = module
    
    tracker.start()
    
    # Step 1: ë² ì´ìŠ¤ë¼ì¸ í‰ê°€
    print("\n" + "ğŸ”µ"*25)
    print("Step 1: ë² ì´ìŠ¤ë¼ì¸ í‰ê°€")
    print("ğŸ”µ"*25)
    baseline_results = evaluate_with_dspy(
        module, test_examples, tracker, "Baseline"
    )
    
    if save_intermediate:
        tracker.plot_metrics()
    
    # Step 2: ìµœì í™” ì‹¤í–‰
    print("\n" + "ğŸŸ¡"*25)
    print(f"Step 2: ìµœì í™” ì‹¤í–‰ ({optimizer_type.upper()})")
    print("ğŸŸ¡"*25)
    
    optimization_start = time.time()
    
    if optimizer_type == "copro":
        # COPRO: Instruction(ì„¤ëª…ë¬¸)ë§Œ ìµœì í™”
        optimized_module = optimize_with_copro(
            module=module,
            train_examples=train_examples,
            metric_fn=normalized_metric,
            breadth=breadth,
            depth=depth,
            num_threads=num_threads,
            init_temperature=init_temperature
        )
    elif optimizer_type == "mipro":
        # MIPROv2: Instruction + Few-shot ëª¨ë‘ ìµœì í™” (ë² ì´ì§€ì•ˆ)
        optimized_module = optimize_with_mipro(
            module=module,
            train_examples=train_examples,
            metric_fn=normalized_metric,
            num_candidates=10,
            init_temperature=1.4,
            max_bootstrapped_demos=3,
            max_labeled_demos=4,
            num_threads=20
        )
    else:
        raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” optimizer_type: {optimizer_type}. 'copro' ë˜ëŠ” 'mipro'ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
    
    optimization_time = time.time() - optimization_start
    print(f"\nâ±ï¸  ìµœì í™” ì‹œê°„: {optimization_time:.1f}ì´ˆ")
    
    # Step 3: ìµœì í™” í›„ í‰ê°€
    print("\n" + "ğŸŸ¢"*25)
    print("Step 3: ìµœì í™” í›„ í‰ê°€")
    print("ğŸŸ¢"*25)
    optimized_results = evaluate_with_dspy(
        optimized_module, test_examples, tracker, "Optimized"
    )
    
    # Step 4: í”„ë¡¬í”„íŠ¸ ë¹„êµ
    print("\n" + "ğŸ”®"*25)
    print("Step 4: í”„ë¡¬í”„íŠ¸ ë³€í™” í™•ì¸")
    print("ğŸ”®"*25)
    compare_prompts(baseline_module, optimized_module)
    # ìµœì¢… ê·¸ë˜í”„ ì €ì¥
    tracker.plot_metrics()
    tracker.save_history()
    tracker.summary()
    return optimized_module, optimized_results


print("âœ… DSPy Evaluate ê¸°ë°˜ í‰ê°€ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ")

if __name__ == "__main__":
        # ============================================
    # 10-1. íŠ¸ë˜í‚¹ì´ í¬í•¨ëœ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    # ============================================
    """
    ğŸ”§ ìµœì í™” ë°©ë²• ì„ íƒ:
    - "copro": Instruction(ì„¤ëª…ë¬¸)ë§Œ ìµœì í™”
    - "mipro": Instruction + Few-shot ëª¨ë‘ ìµœì í™” (ë² ì´ì§€ì•ˆ)
    """

    # âš™ï¸ ì„¤ì •
    OPTIMIZER_TYPE = "copro"  # "copro" ë˜ëŠ” "mipro" ì„ íƒ!
    TRAIN_SIZE = 5           # í•™ìŠµ ë°ì´í„° ìˆ˜
    TEST_SIZE = 10            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìˆ˜

    # ìƒˆ íŠ¸ë˜ì»¤ ìƒì„±
    tracker = PerformanceTracker(f"bird_text2sql_{OPTIMIZER_TYPE}_optimization")

    # ìƒˆ ëª¨ë“ˆ ìƒì„±
    module_for_optimization = TextToSQLModule()
    print(f"ğŸš€ ìµœì í™” ë°©ë²•: {OPTIMIZER_TYPE.upper()}")
    # print(f"ğŸ“Š í•™ìŠµ ë°ì´í„°: {TRAIN_SIZE}ê°œ, í…ŒìŠ¤íŠ¸ ë°ì´í„°: {TEST_SIZE}ê°œ")
    print("="*50)

    optimized_module, final_results = run_optimization_with_tracking(
        module=module_for_optimization,
        train_examples=train_examples,
        test_examples=test_examples,
        tracker=tracker,
        optimizer_type=OPTIMIZER_TYPE,
        save_intermediate=True,
        breadth=3,
        depth=1,
        init_temperature=0.6,
        num_threads=10
    )

    print("\nâœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ê²½ë¡œ: {RESULTS_DIR}")

    # ============================================
    # 10. ë² ì´ìŠ¤ë¼ì¸ í‰ê°€ (ìµœì í™” ì „)
    # ============================================

    # ê¸°ë³¸ ëª¨ë“ˆ ìƒì„±
    baseline_module = TextToSQLModule()

    # í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ëª‡ ê°œë¡œ ë² ì´ìŠ¤ë¼ì¸ í‰ê°€
    print("ğŸ“ ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ í‰ê°€ (ìµœì í™” ì „)")
    print("=" * 50)

    # ì²˜ìŒ 10ê°œë§Œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
    baseline_results = evaluate_model(
        baseline_module, 
        test_examples[:5], 
        verbose=True
    )

    print("ğŸ“ ìµœì í™”ëœ ëª¨ë¸ í‰ê°€")
    print("=" * 50)
    optimized_results = evaluate_model(
        optimized_module, 
        test_examples[:5], 
        verbose=True
    )

    improvement = optimized_results['avg_score'] - baseline_results['avg_score']
    print(f"\nğŸ“ˆ ê°œì„ ìœ¨: {improvement:+.3f} ì ")
    print(f"ë² ì´ìŠ¤ë¼ì¸: {baseline_results['avg_score']:.3f} â†’ ìµœì í™”: {optimized_results['avg_score']:.3f}")
    import json

    SAVE_PATH = "/data/workspace/sogang/sqlagent/optimized_text2sql_model.json"

    def save_optimized_module(module: dspy.Module, path: str):
        """ìµœì í™”ëœ ëª¨ë“ˆ ì €ì¥"""
        module.save(path)
        print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {path}")


    def load_optimized_module(path: str) -> dspy.Module:
        """ì €ì¥ëœ ëª¨ë“ˆ ë¡œë“œ"""
        module = TextToSQLModule()
        module.load(path)
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {path}")
        return module


    # ìµœì í™”ëœ ëª¨ë¸ ì €ì¥
    save_optimized_module(optimized_module, SAVE_PATH)
