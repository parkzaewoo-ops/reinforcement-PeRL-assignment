# 프로젝트: 강화학습을 이용한 자가 개선 SQL 에이전트

## 1. 프로젝트 목표

본 프로젝트의 목표는 LLM(Large Language Model)이 스스로의 실수를 통해 학습하고, 점진적으로 성능을 개선해 나가는 자동화된 프롬프트 튜닝 시스템을 구축하는 것입니다. 사용자의 질문을 SQL 쿼리로 변환하는 Text-to-SQL 과제를 대상으로, 강화학습(Reinforcement Learning)의 핵심 원리를 적용하여 LLM 에이전트가 스스로의 정책(Policy)인 시스템 프롬프트(System Prompt)를 최적화하도록 설계했습니다.

## 2. 핵심 아이디어: 프롬프트를 '자연어 가중치'로

전통적인 머신러닝 모델이 숫자 형태의 가중치(weights)를 업데이트하며 학습하는 반면, 이 프로젝트에서는 LLM의 행동 지침인 **시스템 프롬프트를 '자연어 가중치'로 간주**합니다. 즉, 학습 과정은 숫자 파라미터를 조정하는 대신, 에이전트가 더 나은 행동을 하도록 유도하는 **자연어 규칙을 시스템 프롬프트에 동적으로 추가하고 개선**하는 방식으로 이루어집니다.

이러한 접근은 LLM들 간의 상호작용과 피드백을 통해 이루어지는 **논리적 최적화(Logical Optimization)** 과정이라 할 수 있습니다.

## 3. 강화학습 프레임워크와의 연결

이 시스템은 다음과 같이 강화학습의 핵심 구성 요소에 대응됩니다.

-   **에이전트 (Agent):** SQL 쿼리를 생성하는 LLM (`generate_sql` 노드).
-   **정책 (Policy):** 에이전트의 행동 방식을 결정하는 `시스템 프롬프트`.
-   **행동 (Action):** 주어진 질문에 대해 SQL 쿼리를 생성하는 것.
-   **환경 (Environment):** 생성된 SQL을 평가하고 피드백을 제공하는 LLM (`evaluate_sql` 노드).
-   **보상 (Reward):** SQL의 정확성을 -1.0에서 +1.0 사이의 정량적 점수로 평가한 `reward_score`.
-   **피드백 (Feedback):** 왜 그런 보상 점수를 받았는지, 어떻게 개선해야 하는지에 대한 상세한 자연어 설명. 이는 전통적인 강화학습의 **기울기(Gradient)**와 유사한 역할을 수행하며, 정책 개선의 방향을 제시합니다.
-   **정책 개선 (Policy Improvement):** 낮은 보상을 받았을 때, 피드백과 과거의 실패 기록을 바탕으로 시스템 프롬프트를 업데이트하는 과정 (`update_prompt` 노드).

## 4. 전체 워크플로우

프로젝트는 `LangGraph` 라이브러리를 기반으로 다음과 같은 자동화된 루프를 통해 작동합니다.

![Workflow Diagram](https://i.imgur.com/example.png)  <!-- 이 부분은 실제 생성된 그래프 이미지 경로로 대체될 수 있습니다. -->

1.  **SQL 생성 (Generate SQL):** 에이전트는 현재의 시스템 프롬프트(정책)를 바탕으로 SQL 쿼리(행동)를 생성합니다.
2.  **평가 및 보상 (Evaluate SQL):** 환경은 생성된 SQL을 평가하여 정량적 보상 점수와 질적 피드백을 반환합니다.
3.  **의사 결정 (Decide Next Step):**
    -   보상 점수가 기준치(1.0) 이상이면 **'성공'**으로 간주하고 해당 데이터에 대한 작업을 종료합니다.
    -   보상 점수가 기준치 미만이면 **'실패'**로 간주하고 정책 개선 단계로 넘어갑니다.
    -   실패가 특정 횟수(3회) 이상 반복되면, 시간 낭비를 막기 위해 작업을 중단하고 다음 데이터로 넘어갑니다.
4.  **프롬프트 업데이트 (Update Prompt):** 에이전트는 피드백과 과거의 실패 기록(`failure_history`)을 참조하여, 같은 실수를 반복하지 않도록 시스템 프롬프트에 새로운 지침을 추가합니다.
5.  **루프:** 업데이트된 프롬프트를 가지고 다시 1번 단계로 돌아가 SQL 생성을 재시도합니다.

이러한 **"생성 → 평가 → 개선"** 사이클을 훈련 데이터셋 전체에 대해 반복하며, 시스템 프롬프트는 점진적으로 정교하고 강력하게 진화합니다.

## 5. 실험 설계 및 검증

프로젝트의 객관적인 성능을 검증하기 위해 다음과 같이 실험을 설계했습니다.

-   **데이터 분리:** 전체 데이터셋을 **훈련(80%)**과 **테스트(20%)** 용으로 분리하여, 모델이 학습하지 않은 데이터에 대한 일반화 성능을 측정합니다.
-   **학습 곡선:** 훈련 과정에서 매 에피소드(데이터)마다 얻는 보상 점수를 기록하고, 이를 **학습 곡선(Learning Curve)** 그래프로 시각화하여 에이전트의 성능이 점진적으로 향상되는지를 확인합니다.
-   **최종 성능 평가:** 훈련을 통해 최종적으로 완성된 시스템 프롬프트를 사용하여, 테스트 데이터셋에 대한 **평균 보상 점수**와 **성공률**을 측정하고 리포트합니다.

## 6. 결론

본 프로젝트는 LLM을 단순한 도구가 아닌, 환경과의 상호작용을 통해 스스로 학습하고 발전할 수 있는 **강화학습 에이전트**로 활용하는 새로운 가능성을 제시합니다. 숫자 기반의 최적화를 넘어, 자연어 피드백을 통한 논리적 추론 기반의 정책 개선이 가능함을 보여주며, 이는 향후 더 복잡한 문제 해결을 위한 자율 에이전트 개발에 중요한 기반이 될 수 있습니다.
